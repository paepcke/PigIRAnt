#!/bin/bash

# Script that runs the PigScript buildWebBaseIndex.pig with
# command line paramters. Outputs two files: the index, which
# maps words to occurrence pages and positions within those pages,
# and a map from the docIDs used in the above, to URLs of the
# respective documents. 
#
# Output file names are:
#    <destDir>/<crawlName>_<numPages>_<startSite>_<endSite>_index.idx
#    <destDir>/<crawlName>_<numPages>_<startSite>_<endSite>_urlMap.idx
#
# Defaults are: <destDir>: HDFS home directory of current user
#               <numPages>: 'allPages'
#               <startSite>: 'firstSite'
#               <endSite>: 'lastSite'

USAGE="Usage: buildWebBaseIndex [{-h | --help}] \n
                         \t\t\t [{-v | --version}] \n
                         \t\t\t [{-x | --execmode] {local | mapreduce}] \n
                         \t\t\t [{-d | --destdir} <destinationDirectory>] (default is pwd) \n
                         \t\t\t [{-n | --numpages} <numberOfPages>] (default is all) \n
                         \t\t\t [{-s | --startsite} <startSite>] (default is first site) \n
                         \t\t\t [{-e | --endsite} <endSite>] (default is last site) \n
                         \t\t\t <crawlName> \n
    \tExample: buildWebBaseIndex -d /user/doe/myindex --startsite www.emus.com gov-04-2007 \n
    \tExample: buildWebBaseIndex --numpages 2 gov-04-2007"


PROGVERSION=1.0
EXEC_MODE=mapreduce
# Default destination is the
# cluster's HDFS home directory of
# the user who is issuing the 
# command. So, get the non-HDFS
# $HOME, chop off the last element,
# and prepend our cluster's HDFS
# '/user/' user directory:
DEST_DIR=/user/`basename $HOME`

NUM_PAGES=""
START_SITE=""
END_SITE=""

SHORTOPTS="hvx:d:n:s:e:"
LONGOPTS="help,version,execmode:,destdir:,numpages:,startsite:,endsite:"

ARGS=`getopt -s bash --options=$SHORTOPTS  \
  --longoptions=$LONGOPTS --name=$PROGNAME -- "$@"`

eval set -- "$ARGS"

while true; do
   case $1 in
      -h|--help)
         echo -e $USAGE
         exit 0
         ;;
      -v|--version)
         echo "$PROGVERSION"
	 exit 0
         ;;
      -x|--execmode)
         shift
         EXEC_MODE=$1
         ;;
      -d|--destdir)
         shift
         DEST_DIR=$1
         ;;
      -n|--numpages)
         shift
         NUM_PAGES=$1
         ;;
      -s|--startsite)
         shift
         START_SITE=$1
         ;;
      -e|--endsite)
         shift
         END_SITE=$1
         ;;
      --)
         shift
         break
         ;;
      *)
         shift
         break
         ;;
   esac
   # Throw away the '--' that's added by getopt.
   # The -- case above doesn't get hit when user
   # forgets to put in any required args.
   shift
done

# echo "execMode : '$EXEC_MODE'"
# echo "destDir  : '$DEST_DIR'"
# echo "numPages : '$NUM_PAGES'"
# echo "startSite: '$START_SITE'"
# echo "endSite  : '$END_SITE'"
# echo "crawl: '$1'"

if [ $# == 0 ] 
then
    echo "Missing crawl name."
    echo -e $USAGE
    exit -1
else
    CRAWL_NAME=$1
fi

# If we are running in cygwin, we have to convert the 
# path to the Pig script into a Windows path:

export SCRIPT_DIR=`dirname $0`
if [[ `uname` == *CYGWIN* ]]
then 
  export SCRIPT_DIR=`cygpath --mixed ${SCRIPT_DIR}`
fi

# Construct a path name root for the index.
# Like this: <destDir>/<crawlName>_numPages_startSite_endSite,
# where defaults are made to be: allPages, firstSite, lastSite

if [ -z $NUM_PAGES ]
then
    TMP_NUM_PAGES=allPages
else
    TMP_NUM_PAGES=$NUM_PAGES
fi

if [ -z $START_SITE ]
then
    TMP_START_SITE=firstSite
else
    TMP_START_SITE=$START_SITE
fi
    
if [ -z $END_SITE ]
then
    TMP_END_SITE=lastSite
else
    TMP_END_SITE=$END_SITE
fi
    
ROOT_DEST_NAME=${DEST_DIR}/${CRAWL_NAME}_${TMP_NUM_PAGES}_${TMP_START_SITE}_${TMP_END_SITE}


# Check whether either of the two index target files exists.
# If so, Pig would run for a long time, and then die. Make
# this more fail-fast:

EXISTENCE=`hadoop fs -stat ${DEST_DIR}/${ROOT_DEST_NAME}_urlMap.idx 2> /dev/null` 
if [ -n "$EXISTENCE" ]
then
    echo "File ${DEST_DIR}/${ROOT_DEST_NAME}_urlMap.idx already exists. Quitting."
    exit -1
fi

EXISTENCE=`hadoop fs -stat ${DEST_DIR}/${ROOT_DEST_NAME}_index.idx 2> /dev/null`
if [ -n "$EXISTENCE" ]
then
    echo "File ${DEST_DIR}/${ROOT_DEST_NAME}_index.idx already exists. Quitting."
    exit -1
fi

# Build the Pig STORE commands for storing the docID-->URL
# mapping, and the index itself:
URL_MAP_STORE_COMMAND="STORE URLMap INTO '${DEST_DIR}/${ROOT_DEST_NAME}_urlMap.idx' USING PigStorage;"
INDEX_STORE_COMMAND="STORE sortedPostings INTO '${TARGET_DIR}/${ROOT_DEST_NAME}_index.idx' USING PigStorage;"

# The crawl source is expected in the form: crawlName:numPages:startSite:endSite,
# though numPages, startSite, and endSite are all optional. Rather than 
# doing lots of if statements, we just always include the three, even
# if they are empty. If all three are empty that will be crawlName:::,
# which is fine:

CRAWL_SOURCE=${CRAWL_NAME}:${NUM_PAGES}:${START_SITE}:${END_SITE}
TMP_INDEX_PATH=${DEST_DIR}/${ROOT_DEST_NAME}_index.idx_tmp

pigrun -x $EXEC_MODE \
        URL_MAP_DEST=${DEST_DIR}/${ROOT_DEST_NAME}_urlMap.idx \
        INDEX_DEST=${DEST_DIR}/${ROOT_DEST_NAME}_index.idx \
        TMP_INDEX_DEST=$TMP_INDEX_PATH \
        CRAWL_SOURCE=$CRAWL_SOURCE \
        ${SCRIPT_DIR}/buildWebBaseIndex.pig

EXISTENCE=`hadoop fs -stat $TMP_INDEX_PATH 2> /dev/null`
if [ -n "$EXISTENCE" ]
then
    echo "Removing temp file $TMP_INDEX_PATH."
    `hadoop fs -rmr $TMP_INDEX_PATH &> /dev/null`
    echo Done;
fi

exit 0
